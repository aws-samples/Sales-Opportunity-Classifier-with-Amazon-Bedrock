{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "\n",
    "with open('/home/ec2-user/SageMaker/bucket_name.txt', 'r') as file:\n",
    "    bucket_name = file.read().strip()\n",
    "# S3 bucket and file details\n",
    "bucket_name = bucket_name #Change bucket name to your s3 bucket name\n",
    "file_name = 'sfdc_test.xlsx'\n",
    "boto3.setup_default_session()\n",
    "\n",
    "\n",
    "# Read the custom list from the file\n",
    "with open('/home/ec2-user/SageMaker/custom_list.txt', 'r') as file:\n",
    "    custom_list = file.read().strip().split(',')\n",
    "\n",
    "print(f\"Bucket name: {bucket_name}\")\n",
    "print(f\"Custom list: {custom_list}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize S3 and Bedrock clients\n",
    "try:\n",
    "    s3_client = boto3.client('s3')\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    print(\"Successfully initialized AWS clients\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing AWS clients: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Function to read Excel file from S3\n",
    "def read_excel_from_s3(bucket, key):\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        excel_content = response['Body'].read()\n",
    "        # Use row 0 as header\n",
    "        df = pd.read_excel(io.BytesIO(excel_content), header=0, engine='openpyxl')\n",
    "        print(f\"Successfully read file {key} from bucket {bucket}\")\n",
    "        return df\n",
    "    except ClientError as e:\n",
    "        print(f\"Error reading file from S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Read the Excel file from S3\n",
    "try:\n",
    "    df = read_excel_from_s3(bucket_name, file_name)\n",
    "    print(\"Original DataFrame:\")\n",
    "\n",
    "    # Check if 'Opportunity Name' and 'Description' columns exist\n",
    "    required_columns = ['Opportunity Name', 'Description', 'Opportunity Details']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read Excel file or process DataFrame: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Updated to a single-level list of categories\n",
    "ai_categories = custom_list\n",
    "\n",
    "def categorize_opportunity(description, opportunity_name, opportunity_details):\n",
    "    prompt = f\"\"\"Analyze the following opportunity information and categorize it into one of these AI categories:\n",
    "    {', '.join(ai_categories)}\n",
    "\n",
    "    If it doesn't fit any category or there's not enough information, respond with 'Not Defined'.\n",
    "    Be conservative in your categorization. If you're not sure, choose 'Not Defined'.\n",
    "\n",
    "    Opportunity Name: {opportunity_name}\n",
    "    Description: {description}\n",
    "    Opportunity Details: {opportunity_details}\n",
    "\n",
    "    Respond with only the category name, without any additional text.\n",
    "\n",
    "    Category:\"\"\"\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\"  # Claude 3 Sonnet model ID\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        category = response_body.get('content', [{}])[0].get('text', '').strip()\n",
    "\n",
    "        if category not in ai_categories:\n",
    "            category = 'Not Defined'\n",
    "\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock model for {opportunity_name}: {str(e)}\")\n",
    "        return 'Not Defined'\n",
    "\n",
    "def process_opportunity(row):\n",
    "    try:\n",
    "        opportunity_name = row['Opportunity Name']\n",
    "        description = str(row.get('Description', ''))\n",
    "        opportunity_details = str(row.get('Opportunity Details', ''))\n",
    "\n",
    "        category = categorize_opportunity(description, opportunity_name, opportunity_details)\n",
    "\n",
    "        hashtag = '#' + ''.join(category.lower().split())\n",
    "        tagged_opportunity_name = f\"{opportunity_name} {hashtag}\"\n",
    "\n",
    "        return pd.Series({\n",
    "            'Tagged Opportunity Name': tagged_opportunity_name,\n",
    "            'Category': category\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        return pd.Series({\n",
    "            'Tagged Opportunity Name': opportunity_name + \" #error\",\n",
    "            'Category': \"Error\"\n",
    "        })\n",
    "\n",
    "# Apply the function to create new columns\n",
    "try:\n",
    "    new_columns = df.apply(process_opportunity, axis=1)\n",
    "    df[['Tagged Opportunity Name', 'Category']] = new_columns\n",
    "\n",
    "    print(\"\\nUpdated DataFrame:\")\n",
    "    print(df[['Opportunity Name', 'Tagged Opportunity Name', 'Category']].head().to_string())\n",
    "except Exception as e:\n",
    "    print(f\"Error processing opportunities: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Function to save DataFrame to S3\n",
    "def save_dataframe_to_s3(df, bucket, key):\n",
    "    try:\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Successfully saved file to S3 as {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file to S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Save the updated DataFrame back to S3 as CSV\n",
    "try:\n",
    "    output_file_name = 'tagged_' + file_name.rsplit('.', 1)[0] + '.csv'\n",
    "    save_dataframe_to_s3(df, bucket_name, output_file_name)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save updated file to S3: {str(e)}\")\n",
    "\n",
    "print(\"\\nScript execution completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
